from airflow.decorators import dag, task
from airflow.models import Variable
from datetime import datetime, timedelta
import requests
import logging
import pytz

KST = pytz.timezone('Asia/Seoul')
logger = logging.getLogger(__name__)

FLINK_GATEWAY_URL = "http://sql-gateway-service-20.flink.svc.cluster.local:8083"

@dag(
    dag_id='resync_batch_limited',
    description='RDS ê¸°ì¡´ ë°ì´í„° 5ê°œì”© ìˆœì°¨ ì „ì†¡ (1ë¶„ë§ˆë‹¤)',
    schedule='*/1 * * * *',
    start_date=datetime(2025, 12, 12, tzinfo=KST),
    catchup=False,
    tags=['flink', 'batch', 'resync', 'limited'],
    default_args={
        'owner': 'airflow',
        'depends_on_past': False,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 2,
        'retry_delay': timedelta(minutes=1),
    }
)
def resync_batch_limited():
    
    @task
    def calculate_offset(**context):
        """
        Airflow Variable ê¸°ë°˜ offset ê³„ì‚°
        ë§¤ ì‹¤í–‰ë§ˆë‹¤ +5ì”© ì¦ê°€
        """
        # Variableì—ì„œ í˜„ì¬ offset ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 0ë¶€í„° ì‹œì‘)
        try:
            current_offset = int(Variable.get('resync_batch_offset', default_var='0'))
        except:
            current_offset = 0
            Variable.set('resync_batch_offset', '0')
        
        logger.info(f"ğŸ“Š í˜„ì¬ offset: {current_offset} (í–‰ {current_offset+1}~{current_offset+5})")
        
        # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ offset ì¦ê°€
        next_offset = current_offset + 5
        Variable.set('resync_batch_offset', str(next_offset))
        logger.info(f"â¡ï¸ ë‹¤ìŒ offset: {next_offset} (Variableì— ì €ì¥)")
        
        return current_offset
    
    @task
    def read_sql_file(offset: int):
        """Flink SQL íŒŒì¼ ì½ê¸° ë° offset íŒŒë¼ë¯¸í„° ì£¼ì…"""
        sql_file_path = "/opt/airflow/dags/repo/flink_sql/04_resync_batch_limited.sql"
        
        try:
            with open(sql_file_path, 'r', encoding='utf-8-sig') as f:
                sql_content = f.read()
            
            # offset íŒŒë¼ë¯¸í„° ì£¼ì… (ìˆœì„œ ì¤‘ìš”: offset_end ë¨¼ì €!)
            offset_start = offset
            offset_end = offset + 5
            
            sql_content = sql_content.replace(':offset_end', str(offset_end))
            sql_content = sql_content.replace(':offset', str(offset_start))
            
            logger.info(f"SQL íŒŒì¼ ì½ê¸° ì„±ê³µ: {sql_file_path}")
            logger.info(f"ì²˜ë¦¬ ë²”ìœ„: rn > {offset_start} AND rn <= {offset_end}")
            return sql_content
            
        except Exception as e:
            logger.error(f"SQL íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            raise
    
    @task
    def submit_batch_job(sql_content: str):
        """Flink Batch Job ì œì¶œ"""
        # 1. ì„¸ì…˜ ìƒì„±
        session_url = f"{FLINK_GATEWAY_URL}/v1/sessions"
        
        try:
            session_response = requests.post(session_url, json={}, timeout=10)
            session_response.raise_for_status()
            session_handle = session_response.json()['sessionHandle']
            logger.info(f"ì„¸ì…˜ ìƒì„± ì„±ê³µ: {session_handle}")
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
        
        # 2. SQL êµ¬ë¬¸ ë¶„ë¦¬
        url = f"{FLINK_GATEWAY_URL}/v1/sessions/{session_handle}/statements"
        statements = []
        current_statement = ""
        
        for line in sql_content.split('\n'):
            line = line.strip()
            if line.startswith('--') or not line:
                continue
            current_statement += line + " "
            if line.endswith(';'):
                statements.append(current_statement.strip())
                current_statement = ""
        
        if current_statement.strip():
            statements.append(current_statement.strip())
        
        logger.info(f"ì´ {len(statements)}ê°œì˜ SQL êµ¬ë¬¸ ê°ì§€")
        
        # 3. SQL ì‹¤í–‰
        try:
            for idx, stmt in enumerate(statements, 1):
                logger.info(f"[{idx}/{len(statements)}] SQL ì‹¤í–‰ ì¤‘...")
                
                # INSERT êµ¬ë¬¸ì€ ì „ì²´ SQL ì¶œë ¥
                if stmt.strip().upper().startswith('INSERT'):
                    logger.info(f"ì „ì²´ SQL: {stmt}")
                else:
                    stmt_preview = stmt[:200] if len(stmt) > 200 else stmt
                    logger.info(f"SQL Preview: {stmt_preview}...")
                
                response = requests.post(url, json={"statement": stmt}, timeout=30)
                response.raise_for_status()
                
                operation_handle = response.json()['operationHandle']
                
                # INSERT êµ¬ë¬¸ ì™„ë£Œ ëŒ€ê¸°
                if stmt.strip().upper().startswith('INSERT'):
                    import time
                    max_wait = 120
                    waited = 0
                    
                    while waited < max_wait:
                        status_url = f"{FLINK_GATEWAY_URL}/v1/sessions/{session_handle}/operations/{operation_handle}/status"
                        status_response = requests.get(status_url, timeout=10)
                        status = status_response.json().get('status')
                        
                        if status == 'FINISHED':
                            logger.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
                            break
                        elif status == 'ERROR':
                            # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ë¡œê·¸
                            error_response = status_response.json()
                            logger.error(f"âŒ Flink SQL ì—ëŸ¬ ë°œìƒ:")
                            logger.error(f"Status Response: {error_response}")
                            error_msg = error_response.get('error', {})
                            if isinstance(error_msg, dict):
                                logger.error(f"Error Message: {error_msg.get('message', 'No message')}")
                                logger.error(f"Error Type: {error_msg.get('type', 'No type')}")
                                logger.error(f"Stack Trace: {error_msg.get('stack', 'No stack')}")
                            else:
                                logger.error(f"Error: {error_msg}")
                            raise Exception(f"SQL ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
                        
                        time.sleep(2)
                        waited += 2
                    
                    if waited >= max_wait:
                        logger.warning(f"âš ï¸ [{idx}/{len(statements)}] íƒ€ì„ì•„ì›ƒ")
                else:
                    logger.info(f"âœ… [{idx}/{len(statements)}] ì™„ë£Œ!")
            
            logger.info("âœ… ëª¨ë“  SQL ì‹¤í–‰ ì™„ë£Œ (ê° í…Œì´ë¸”ë‹¹ 5ê°œì”©)")
            
        except Exception as e:
            logger.error(f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
        
        finally:
            # 4. ì„¸ì…˜ ì¢…ë£Œ
            try:
                delete_url = f"{FLINK_GATEWAY_URL}/v1/sessions/{session_handle}"
                requests.delete(delete_url, timeout=10)
                logger.info(f"ì„¸ì…˜ ì¢…ë£Œ: {session_handle}")
            except Exception as e:
                logger.warning(f"ì„¸ì…˜ ì¢…ë£Œ ì‹¤íŒ¨: {str(e)}")
    
    # DAG ì‹¤í–‰ ìˆœì„œ
    offset = calculate_offset()
    sql_content = read_sql_file(offset)
    submit_batch_job(sql_content)

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
resync_batch_limited_dag = resync_batch_limited()
